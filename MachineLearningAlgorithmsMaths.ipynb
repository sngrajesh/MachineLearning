{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Large\\text{Machine Learning Algorithms}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithms\n",
    "\n",
    "## **Supervised Learning Algorithms**\n",
    "\n",
    "### **1. Linear Regression**\n",
    "Used for predicting continuous values.\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1X + \\epsilon\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $Y$ = dependent variable  \n",
    "- $X$ = independent variable  \n",
    "- $\\beta_0$ = intercept  \n",
    "- $\\beta_1$ = coefficient  \n",
    "- $\\epsilon$ = error term\n",
    "\n",
    "Cost Function:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x_i) - y_i)^2\n",
    "$$\n",
    "\n",
    "Gradient Descent Update Rule:\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Logistic Regression**\n",
    "Used for binary classification.\n",
    "\n",
    "$$\n",
    "h_{\\theta}(x) = \\frac{1}{1 + e^{-\\theta^T x}}\n",
    "$$\n",
    "\n",
    "Loss Function (Binary Cross-Entropy):\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[y_i \\log(h_{\\theta}(x_i)) + (1 - y_i) \\log(1 - h_{\\theta}(x_i))\\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Support Vector Machine (SVM)**\n",
    "Used for classification.\n",
    "\n",
    "Decision Boundary:\n",
    "\n",
    "$$\n",
    "w^T x + b = 0\n",
    "$$\n",
    "\n",
    "Hinge Loss:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{2} ||w||^2 + C \\sum_{i=1}^{m} \\max(0, 1 - y_i (w^T x_i + b))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **4. k-Nearest Neighbors (KNN)**\n",
    "Used for classification and regression.\n",
    "\n",
    "Distance Metric (Euclidean):\n",
    "\n",
    "$$\n",
    "d(p, q) = \\sqrt{\\sum_{i=1}^{n} (p_i - q_i)^2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Naive Bayes Classifier**\n",
    "Used for classification.\n",
    "\n",
    "Bayes Theorem:\n",
    "\n",
    "$$\n",
    "P(Y|X) = \\frac{P(X|Y) P(Y)}{P(X)}\n",
    "$$\n",
    "\n",
    "Gaussian Naive Bayes:\n",
    "\n",
    "$$\n",
    "P(X_i | Y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(X_i - \\mu)^2}{2\\sigma^2}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Decision Tree**\n",
    "Used for classification and regression.\n",
    "\n",
    "Entropy:\n",
    "\n",
    "$$\n",
    "H(S) = -\\sum_{i=1}^{n} p_i \\log_2 p_i\n",
    "$$\n",
    "\n",
    "Gini Impurity:\n",
    "\n",
    "$$\n",
    "Gini = 1 - \\sum_{i=1}^{n} p_i^2\n",
    "$$\n",
    "\n",
    "Information Gain:\n",
    "\n",
    "$$\n",
    "IG = H(S) - \\sum_{j=1}^{m} \\frac{|S_j|}{|S|} H(S_j)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Random Forest**\n",
    "An ensemble of decision trees.\n",
    "\n",
    "Prediction:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\frac{1}{n} \\sum_{i=1}^{n} h_i(x)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Gradient Boosting (GBM) & XGBoost**\n",
    "Boosting algorithm that minimizes loss iteratively.\n",
    "\n",
    "Update Rule:\n",
    "\n",
    "$$\n",
    "F_m(x) = F_{m-1}(x) + \\gamma h_m(x)\n",
    "$$\n",
    "\n",
    "$\\gamma$ is the learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "## **Unsupervised Learning Algorithms**\n",
    "\n",
    "### **9. k-Means Clustering**\n",
    "Used for clustering.\n",
    "\n",
    "Centroid Update:\n",
    "\n",
    "$$\n",
    "\\mu_j = \\frac{1}{|C_j|} \\sum_{x_i \\in C_j} x_i\n",
    "$$\n",
    "\n",
    "Distance Metric:\n",
    "\n",
    "$$\n",
    "d(x, \\mu) = ||x - \\mu||^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Principal Component Analysis (PCA)**\n",
    "Used for dimensionality reduction.\n",
    "\n",
    "Eigenvalue Equation:\n",
    "\n",
    "$$\n",
    "X^T X v = \\lambda v\n",
    "$$\n",
    "\n",
    "Projection:\n",
    "\n",
    "$$\n",
    "Z = XW\n",
    "$$\n",
    "\n",
    "Where $W$ consists of top eigenvectors.\n",
    "\n",
    "---\n",
    "\n",
    "### **11. Hierarchical Clustering**\n",
    "Distance Between Clusters:\n",
    "\n",
    "- Single Linkage: $d(A, B) = \\min (d(a, b))$\n",
    "- Complete Linkage: $d(A, B) = \\max (d(a, b))$\n",
    "- Average Linkage: $d(A, B) = \\frac{1}{|A||B|} \\sum d(a, b)$\n",
    "\n",
    "---\n",
    "\n",
    "## **Reinforcement Learning Algorithms**\n",
    "\n",
    "### **12. Q-Learning**\n",
    "Used in reinforcement learning.\n",
    "\n",
    "Q-Value Update Rule:\n",
    "\n",
    "$$\n",
    "Q(s, a) = Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ = learning rate\n",
    "- $\\gamma$ = discount factor\n",
    "- $r$ = reward\n",
    "- $s'$ = next state\n",
    "\n",
    "---\n",
    "\n",
    "### **13. Deep Q Network (DQN)**\n",
    "Uses neural networks to approximate Q-values.\n",
    "\n",
    "Loss Function:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\mathbb{E} \\left[ (r + \\gamma \\max_{a'} Q(s', a'; \\theta^-) - Q(s, a; \\theta))^2 \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **14. Policy Gradient Methods**\n",
    "Used for learning policies directly.\n",
    "\n",
    "Policy Gradient Update:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta + \\alpha \\nabla_{\\theta} J(\\theta)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E} [ R ]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Neural Networks and Deep Learning**\n",
    "\n",
    "### **15. Artificial Neural Networks (ANN)**\n",
    "Forward Propagation:\n",
    "\n",
    "$$\n",
    "a^{[l]} = g(W^{[l]} a^{[l-1]} + b^{[l]})\n",
    "$$\n",
    "\n",
    "Backpropagation (Gradient Descent):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W^{[l]}} = \\frac{\\partial J}{\\partial a^{[l]}} \\cdot \\frac{\\partial a^{[l]}}{\\partial W^{[l]}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **16. Convolutional Neural Networks (CNN)**\n",
    "Used for image processing.\n",
    "\n",
    "Convolution Operation:\n",
    "\n",
    "$$\n",
    "S(i, j) = \\sum_{m} \\sum_{n} X(i+m, j+n) \\cdot K(m, n)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **17. Recurrent Neural Networks (RNN)**\n",
    "Used for sequential data.\n",
    "\n",
    "Hidden State Update:\n",
    "\n",
    "$$\n",
    "h_t = \\sigma(W_h h_{t-1} + W_x x_t + b)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **18. Long Short-Term Memory (LSTM)**\n",
    "Used for long-term dependencies in sequences.\n",
    "\n",
    "Cell State Update:\n",
    "\n",
    "$$\n",
    "C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t\n",
    "$$\n",
    "\n",
    "Forget Gate:\n",
    "\n",
    "$$\n",
    "f_t = \\sigma(W_f [h_{t-1}, x_t] + b_f)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Generative Models**\n",
    "\n",
    "### **19. Generative Adversarial Networks (GANs)**\n",
    "Generator Loss:\n",
    "\n",
    "$$\n",
    "L_G = -\\mathbb{E} [\\log D(G(z))]\n",
    "$$\n",
    "\n",
    "Discriminator Loss:\n",
    "\n",
    "$$\n",
    "L_D = -\\mathbb{E} [\\log D(x)] - \\mathbb{E} [\\log (1 - D(G(z)))]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **20. Variational Autoencoders (VAE)**\n",
    "Loss Function:\n",
    "\n",
    "$$\n",
    "L = \\mathbb{E}_{q(z|x)} [\\log p(x|z)] - D_{KL}(q(z|x) || p(z))\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
